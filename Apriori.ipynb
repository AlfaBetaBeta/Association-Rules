{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule Mining - Pipeline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a basic pipeline to perform association rule analysis on a read-in dataset, based on the apriori algorithm. The main steps are elaborated below:\n",
    "\n",
    "* [Imports](#imports)\n",
    "* [Path and input settings](#pathinput)\n",
    "* [Notes](#notes)\n",
    "* [Functions](#functions)\n",
    "* [Execution](#exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path and input settings <a name=\"pathinput\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = \"./\"\n",
    "my_file = \"Training_courses_2019.xlsx\"\n",
    "sheet_name = \"courses\"\n",
    "\n",
    "path_dict = {\n",
    "    \"lift\": my_path + \"Metric-lift/\",\n",
    "    \"confidence ratio\": my_path + \"Metric-confidence-ratio/\",\n",
    "    \"confidence difference\": my_path + \"Metric-confidence-diff/\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(my_path + my_file, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKET</th>\n",
       "      <th>Decision Trees</th>\n",
       "      <th>Building Predictive Models</th>\n",
       "      <th>Intro to CHAID</th>\n",
       "      <th>Classification and Clustering</th>\n",
       "      <th>Data Entry</th>\n",
       "      <th>FastTrack</th>\n",
       "      <th>Intermediate Techniques</th>\n",
       "      <th>Intro to Statistic platform</th>\n",
       "      <th>Intro to Platform &amp; Statistics</th>\n",
       "      <th>Maps</th>\n",
       "      <th>Perceptual Mapping</th>\n",
       "      <th>Market Segmentation</th>\n",
       "      <th>Neural Networks</th>\n",
       "      <th>Scripting</th>\n",
       "      <th>Intro to Statistics</th>\n",
       "      <th>Tables</th>\n",
       "      <th>Time Series</th>\n",
       "      <th>ANOVA models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TICKET  Decision Trees  Building Predictive Models  Intro to CHAID  \\\n",
       "0       2               0                           1               0   \n",
       "1      16               0                           0               0   \n",
       "2      32               0                           0               0   \n",
       "3      33               0                           0               0   \n",
       "4      36               0                           0               1   \n",
       "\n",
       "   Classification and Clustering  Data Entry  FastTrack  \\\n",
       "0                              0           0          0   \n",
       "1                              0           0          0   \n",
       "2                              0           0          0   \n",
       "3                              0           0          0   \n",
       "4                              0           0          0   \n",
       "\n",
       "   Intermediate Techniques  Intro to Statistic platform  \\\n",
       "0                        0                            1   \n",
       "1                        1                            0   \n",
       "2                        0                            1   \n",
       "3                        0                            1   \n",
       "4                        1                            1   \n",
       "\n",
       "   Intro to Platform & Statistics  Maps  Perceptual Mapping  \\\n",
       "0                               0     0                   0   \n",
       "1                               0     0                   0   \n",
       "2                               0     0                   0   \n",
       "3                               0     0                   0   \n",
       "4                               0     0                   0   \n",
       "\n",
       "   Market Segmentation  Neural Networks  Scripting  Intro to Statistics  \\\n",
       "0                    0                0          0                    0   \n",
       "1                    1                0          0                    0   \n",
       "2                    0                0          0                    1   \n",
       "3                    0                0          0                    1   \n",
       "4                    1                0          0                    1   \n",
       "\n",
       "   Tables  Time Series  ANOVA models  \n",
       "0       0            0             0  \n",
       "1       0            0             0  \n",
       "2       0            0             0  \n",
       "3       0            0             0  \n",
       "4       0            1             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = \"lift\"        # controlling metric used for thresholding\n",
    "# min_threshold = 1      # metric threshold\n",
    "\n",
    "# min_rulesupport = 0    # minimum rule support\n",
    "\n",
    "# min_support = 0.2      # minimum antecedent support\n",
    "# min_confidence = 0.3   # minimum confidence\n",
    "\n",
    "max_antecedents = 5    # maximum size of antecedent set\n",
    "max_consequents = 1    # maximum size of consequent set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes <a name=\"notes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE1**: The `pandas` documentation recommends using `DataFrame.to_numpy()` instead of `DataFrame.values`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE2**: From the python documentation, `itertools.combinations(iterable, r)` returns `r` length subsequences of elements from the input `iterable`. Combinations are emitted in lexicographic sort order. So, if the input `iterable` is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE3**: The input to the `metric_dict` lambda functions in `association_rules()` is always a tuple with the following three support values:\n",
    "\n",
    "* `support_tuple[0]`: support( *antecedent* &#8594; *consequent* )\n",
    "\n",
    "* `support_tuple[1]`: support( *antecedent* )\n",
    "\n",
    "* `support_tuple[2]`: support( *consequent* )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE4**: The purpose of `np.vectorize` is to transform functions which are not numpy-aware (e.g. take floats as input and return floats as output) into functions that can operate on (and return) numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE5**: A `frozenset` is immutable, and its items are not intrinsically ordered, which is a convenient feature for the keys of a dictionary (e.g. `frequent_items_dict`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions <a name=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(df, min_rulesupport=0, use_colnames=False,\n",
    "            max_len=None, col_start=1):\n",
    "    \"\"\"\n",
    "    Input arguments:\n",
    "    * `df`\n",
    "      pandas DataFrame with one-hot encoded columns embedding transactional information. The values in\n",
    "      these columns are expected to be 1/0 or True/False. If there are columns with non-transactional\n",
    "      information, these are assumed to be grouped in `df.columns[:col_start]`. See argument `col_start`\n",
    "      for more details.\n",
    "    * `min_rulesupport`\n",
    "      Float representing the minimum threshold for the support value of the itemset under consideration\n",
    "      to be considered frequent. By default (0), no minimum is imposed, which leaves `max_len` as the\n",
    "      only way to narrow down the itemset search. In any case, filtering is enabled when resorting to\n",
    "      metric thresholds upon calling the function `association_rules()`.\n",
    "    * `use_colnames`\n",
    "      Boolean indicating if the indices of the item columns pertaining to itemsets should be handled as\n",
    "      integers (False, default) or with their string names (True).\n",
    "    * `max_len`\n",
    "      Integer imposing a maximum itemset size when searching for frequent itemsets. The search, if not\n",
    "      interrupted by then due to non-compliance with `min_rulesupport`, will not continue for itemsets\n",
    "      larger than this specified size. By default (None) no size restraint is imposed and hence only\n",
    "      `min_rulesupport` could terminate the search before exhausting all possible itemsets.\n",
    "    * `col_start`\n",
    "      Index of the column from which all remaining columns are transactional, i.e. number of columns\n",
    "      with non-transactional information. These columns are ignored throughout the process of searching\n",
    "      for frequent itemsets. By default (1) it is assumed that only one such column exists in `df` (the very\n",
    "      first one).\n",
    "    \n",
    "    Returns:\n",
    "    * pandas DataFrame with two columns (['support', 'itemsets']) collecting the frequent itemsets compliant\n",
    "      with the search conditions as well as their corresponding support values.\n",
    "    \"\"\"\n",
    "    # See Note1\n",
    "    X = df.to_numpy()\n",
    "    column_indices = np.arange(col_start, X.shape[1])\n",
    "    nrows, ncols = X.shape[0], X.shape[1]-col_start\n",
    "    \n",
    "    support = X[:,col_start:].sum(axis=0) / nrows\n",
    "    \n",
    "    support_dict = {1: support[support >= min_rulesupport]}\n",
    "    itemset_dict = {1: column_indices[support >= min_rulesupport].reshape(-1, 1)}\n",
    "    \n",
    "    max_itemset = 1\n",
    "    \n",
    "    while max_itemset and max_itemset < (max_len or float('inf')):\n",
    "        next_max_itemset = max_itemset + 1\n",
    "        # See Note2\n",
    "        combos = combinations(np.unique(itemset_dict[max_itemset].flatten()),\n",
    "                              r=next_max_itemset)\n",
    "        frequent_items = []\n",
    "        frequent_items_support = []\n",
    "\n",
    "        for c in combos:\n",
    "            together = X[:, c].sum(axis=1) == len(c)\n",
    "            support = together.sum() / nrows\n",
    "            if support >= min_rulesupport:\n",
    "                frequent_items.append(c)\n",
    "                frequent_items_support.append(support)\n",
    "\n",
    "        if frequent_items:\n",
    "            itemset_dict[next_max_itemset] = np.array(frequent_items)\n",
    "            support_dict[next_max_itemset] = np.array(frequent_items_support)\n",
    "            max_itemset = next_max_itemset\n",
    "        else:\n",
    "            max_itemset = 0\n",
    "        \n",
    "    all_res = []\n",
    "    \n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([i for i in itemset_dict[k]])\n",
    "\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = ['support', 'itemsets']\n",
    "    \n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df['itemsets'] = res_df['itemsets'].apply(lambda x: [mapping[i] for i in x])\n",
    "        \n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(df_freq,\n",
    "                      metric=\"lift\", min_threshold=1,\n",
    "                      max_antecedents=None, max_consequents=None,\n",
    "                      min_confidence=0.8, min_support=0.1):\n",
    "    \"\"\"\n",
    "    Input arguments:\n",
    "    * `df_freq`\n",
    "      pandas DataFrame with two columns (['support', 'itemsets']) as per output from function `apriori()`.\n",
    "    * `metric`\n",
    "      String signifying the controlling metric by which to filter rules already compliant with support and\n",
    "      confidence thresholds. The mined rules are sorted by this metric in decreasing order. By default,\n",
    "      \"lift\" is assumed as the controlling metric. See `metric_dict` definition to inspect available metrics.\n",
    "    * `min_threshold`\n",
    "      Minimum value that `metric` needs to attain for a rule to be considered and returned. By default it\n",
    "      is set to 1, associated to the default metric \"lift\".\n",
    "    * `max_antecedents`\n",
    "      Maximum size of the antecedent set considered for any rule. By default (None), no limitation is imposed\n",
    "      on this size, which means that all possible antecedent sets within each itemset are exhausted when\n",
    "      mining rules.\n",
    "    * `max_consequents`\n",
    "      Maximum size of the consequent set considered for any rule. By default (None), no limitation is imposed\n",
    "      on this size, which means that all possible consequent sets within each itemset are exhausted when\n",
    "      mining rules.\n",
    "    * `min_confidence`\n",
    "      Minimum value that the confidence of a rule needs to attain for it to be considered and returned.\n",
    "      Defaults to 0.8.\n",
    "    * `min_support`\n",
    "      Minimum value that the support of the antecedent needs to attain for a rule to be considered and returned.\n",
    "      Defaults to 0.1.\n",
    "    \n",
    "    Returns:\n",
    "    * pandas DataFrame with all the mined association rules compliant with the thresholds (if no rules could be\n",
    "      found for the given set of input arguments, a None object is returned instead). Each row represents a\n",
    "      rule, and the columns are intuitively labelled as:\n",
    "          [\"antecedent(s)\",\n",
    "           \"consequent(s)\",\n",
    "           \"antecedent frequency\",\n",
    "           \"antecedent support\",\n",
    "           \"consequent support\", \n",
    "           \"confidence\",\n",
    "           \"rule support\",\n",
    "           \"lift\",\n",
    "           \"confidence difference\",\n",
    "           \"confidence ratio\",\n",
    "           \"size antec.set\",\n",
    "           \"size conseq.set\",\n",
    "           \"rule ID\"]\n",
    "    \"\"\"\n",
    "    # See Note3\n",
    "    metric_dict = {\n",
    "        \"antecedent support\": lambda t: t[1],\n",
    "        \"consequent support\": lambda t: t[2], #Prior\n",
    "        \"confidence\": lambda t: t[0]/t[1],    #Posterior\n",
    "        \"rule support\": lambda t: t[0],\n",
    "        \"confidence difference\": lambda t: abs(t[0]/t[1] - t[2]),\n",
    "        \"confidence ratio\": lambda t: 1 - np.minimum(t[0]/t[1], t[2])/np.maximum(t[0]/t[1], t[2]),\n",
    "        \"lift\": lambda t: (t[0]/t[1])/t[2]\n",
    "    }\n",
    "\n",
    "    columns_ordered = [\n",
    "        \"antecedent support\",\n",
    "        \"consequent support\", \n",
    "        \"confidence\",\n",
    "        \"rule support\",\n",
    "        \"lift\",\n",
    "        \"confidence difference\",\n",
    "        \"confidence ratio\"\n",
    "        ]\n",
    "\n",
    "    # See Note1\n",
    "    keys = df_freq['itemsets'].to_numpy()\n",
    "    values = df_freq['support'].to_numpy()\n",
    "    # See Note4 and Note5\n",
    "    frozenset_vect = np.vectorize(lambda x: frozenset(x))\n",
    "    frequent_items_dict = dict(zip(frozenset_vect(keys), values))\n",
    "    \n",
    "    rule_antecedents = []\n",
    "    rule_consequents = []\n",
    "    rule_supports = []\n",
    "\n",
    "    n_transactions = len(df.index)\n",
    "\n",
    "    for k in frequent_items_dict.keys():\n",
    "        sAC = frequent_items_dict[k]\n",
    "        # Adjust iteration boundaries to the restraints on antecedent/consequent set size\n",
    "        if max_antecedents:\n",
    "            sizeA = min(len(k)-1, max_antecedents)\n",
    "        else:\n",
    "            sizeA = len(k)-1\n",
    "        if max_consequents:\n",
    "            sizeC = max(len(k) - 1 - max_consequents, 0)\n",
    "        else:\n",
    "            sizeC = 0\n",
    "        \n",
    "        for idx in range(sizeA, sizeC, -1):\n",
    "            for c in combinations(k, r=idx):\n",
    "                antecedent = frozenset(c)\n",
    "                consequent = k.difference(antecedent)\n",
    "\n",
    "                sA = frequent_items_dict[antecedent]\n",
    "                sC = frequent_items_dict[consequent]\n",
    "                instances = sA * n_transactions\n",
    "\n",
    "                support_tuple = (sAC, sA, sC)\n",
    "                # Filtering off itemsets without support for antecendent and/or consequent\n",
    "                if not sC or not sA: continue\n",
    "                    \n",
    "                if metric_dict['antecedent support'](support_tuple) >= min_support:\n",
    "                    if metric_dict['confidence'](support_tuple) >= min_confidence:\n",
    "                        if metric_dict[metric](support_tuple) >= min_threshold:\n",
    "                            rule_antecedents.append(antecedent)\n",
    "                            rule_consequents.append(consequent)\n",
    "                            rule_supports.append([len(antecedent), len(consequent), instances, sAC, sA, sC])\n",
    "                            \n",
    "    if not rule_supports:\n",
    "        print (\"WARNING: No rules were found with the specified thresholds.\")\n",
    "        return None\n",
    "    \n",
    "    rule_supports = np.array(rule_supports).T.astype(float)\n",
    "    numbantec = rule_supports[0]\n",
    "    numbconseq = rule_supports[1]\n",
    "    instances = rule_supports[2].astype(int)\n",
    "    sAC = rule_supports[3]\n",
    "    sA = rule_supports[4]\n",
    "    sC = rule_supports[5]\n",
    "    support_tuple = (sAC, sA, sC)\n",
    "\n",
    "    dfrules = pd.DataFrame(data=list(zip(rule_antecedents, rule_consequents, instances)),\n",
    "                           columns=[\"antecedent(s)\", \"consequent(s)\", \"antecedent frequency\"])\n",
    "    \n",
    "    # Cast the frozensets to strings when using the colnames\n",
    "    try:\n",
    "        dfrules[\"antecedent(s)\"] = dfrules[\"antecedent(s)\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "        dfrules[\"consequent(s)\"] = dfrules[\"consequent(s)\"].apply(lambda x: ', '.join(list(x))).astype(\"unicode\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    for m in columns_ordered:\n",
    "        dfrules[m] = metric_dict[m](support_tuple)\n",
    "\n",
    "    dfrules[\"size antec.set\"] = numbantec.astype(int)\n",
    "    dfrules[\"size conseq.set\"] = numbconseq.astype(int)\n",
    "\n",
    "    dfrules = dfrules.sort_values(metric, ascending=False)\n",
    "\n",
    "    dfrules[\"rule ID\"] = dfrules.index + 1\n",
    "    number = [\"s\", \"\"][len(dfrules) == 1]\n",
    "    print(\"INFO: {} rule\".format(str(len(dfrules))) + number + \" found.\")\n",
    "    \n",
    "    return dfrules                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_mining(df,\n",
    "                min_rulesupport=0, use_colnames=False,\n",
    "                max_len=None, col_start=1,\n",
    "                metric=\"lift\", min_threshold=1,\n",
    "                max_antecedents=None, max_consequents=None,\n",
    "                min_confidence=0.8, min_support=0.1):\n",
    "    \"\"\"\n",
    "    Input arguments as per functions `apriori()` and `association_rules()`.\n",
    "    Returns the pandas DataFrame resulting from `association_rules()`.\n",
    "    \n",
    "    This function acts as wrapper and interface between `apriori()` and `association_rules()`, making\n",
    "    sure that the kwargs passed to these functions are consistent when changed from their default values,\n",
    "    particularly those that default to None.\n",
    "    \"\"\"\n",
    "    # Basic sanity checks amongst input arguments\n",
    "    if max_len and max_antecedents and max_consequents:\n",
    "        if (max_antecedents + max_consequents) != max_len:\n",
    "            print(\"\"\"ERROR: For consistency purposes between the size of the frequent itemsets retrieved\n",
    "            and their split into antecedent and consequent with size restraints, `max_len` has to be equal\n",
    "            to the sum of `max_antecedents` and `max_consequents` if all three arguments are specified.\n",
    "            Adjust their values accordingly or leave `max_len` as `None`.\"\"\")\n",
    "            return None\n",
    "        \n",
    "    if max_len is None and max_antecedents and max_consequents:\n",
    "        print(\"\"\"INFO: `max_len` will be set to {} to avoid searching for itemsets beyond the given\n",
    "        restraints on the number of antecedents and consequents.\"\"\".format(max_antecedents + max_consequents))\n",
    "        max_len = max_antecedents + max_consequents\n",
    "        \n",
    "    if max_len and max_antecedents and not max_consequents:\n",
    "        if not max_antecedents < max_len:\n",
    "            print(\"\"\"INFO: Although `max_antecedents` has been set to {0}, only antecedent sets\n",
    "            of size up to {1} will be considered, consistently with given `max_len`. If this is not\n",
    "            intended behaviour, adjust the value of the relevant argument.\"\"\".format(max_antecedents, max_len-1))\n",
    "            \n",
    "    if max_len and not max_antecedents and max_consequents:\n",
    "        if not max_consequents < max_len:\n",
    "            print(\"\"\"INFO: Although `max_consequents` has been set to {0}, only consequent sets\n",
    "            of size up to {1} will be considered, consistently with given `max_len`. If this is not\n",
    "            intended behaviour, adjust the value of the relevant argument.\"\"\".format(max_consequents, max_len-1))\n",
    "            \n",
    "    if max_len and not max_antecedents and not max_consequents:\n",
    "        if max_len > len(df.columns[col_start:]):\n",
    "            print(\"\"\"INFO: Although `max_len` has been set to {0}, only {1} columns of the given\n",
    "            DataFrame are available and hence this constitutes the largest itemset size that will\n",
    "            be considered. Check the value of `col_start`, otherwise set `max_len` to None or adjust\n",
    "            its value appropriately.\"\"\".format(max_len, len(df.columns[col_start:])))\n",
    "            \n",
    "    if max_len is None and max_antecedents and not max_consequents:\n",
    "        if max_antecedents > len(df.columns[col_start:]):\n",
    "            print(\"\"\"INFO: Although `max_antecedents` has been set to {0}, only {1} columns of the\n",
    "            given DataFrame are available and hence this constitutes the largest itemset size that will\n",
    "            be considered. Check the value of `col_start`, otherwise set `max_antecedents` to None or\n",
    "            adjust its value appropriately.\"\"\".format(max_antecedents, len(df.columns[col_start:])))\n",
    "            \n",
    "    if max_len is None and not max_antecedents and max_consequents:\n",
    "        if max_consequents > len(df.columns[col_start:]):\n",
    "            print(\"\"\"INFO: Although `max_consequents` has been set to {0}, only {1} columns of the\n",
    "            given DataFrame are available and hence this constitutes the largest itemset size that will\n",
    "            be considered. Check the value of `col_start`, otherwise set `max_consequents` to None or\n",
    "            adjust its value appropriately.\"\"\".format(max_consequents, len(df.columns[col_start:])))\n",
    "    \n",
    "    # Call function to extract frequent itemsets\n",
    "    support_itemsets = apriori(df, min_rulesupport, use_colnames, max_len, col_start)\n",
    "    \n",
    "    # Call function to mine rules \n",
    "    rules = association_rules(support_itemsets,\n",
    "                              metric, min_threshold,\n",
    "                              max_antecedents, max_consequents,\n",
    "                              min_confidence, min_support)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution <a name=\"exe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 1 rule found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 4 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 9 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 6 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 17 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 31 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 1 rule found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 4 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 12 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 6 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 17 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 38 rules found.\n"
     ]
    }
   ],
   "source": [
    "metric = \"lift\"\n",
    "lift_grid = [1.0]*6 + [0.5]*6\n",
    "rulesupport_grid = [0]*12\n",
    "support_grid = ([0.2]*3 + [0.1]*3) * 2\n",
    "confidence_grid = [0.5, 0.4, 0.3]*4\n",
    "\n",
    "for i, (l, rs, s, c) in enumerate(zip(lift_grid, rulesupport_grid, support_grid, confidence_grid)):\n",
    "    tmp = rule_mining(df,\n",
    "                      use_colnames=True, min_rulesupport=rs,\n",
    "                      metric=metric, min_threshold=l,\n",
    "                      max_antecedents=max_antecedents, max_consequents=max_consequents,\n",
    "                      min_confidence=c, min_support=s)\n",
    "    \n",
    "    tmp_path = path_dict[metric] + metric + \"_c\" + str(i+1) + \".csv\"\n",
    "    tmp.to_csv(tmp_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 870 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 958 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 1113 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 444 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 482 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 508 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 221 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 226 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 238 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 18 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 23 rules found.\n",
      "INFO: `max_len` will be set to 6 to avoid searching for itemsets beyond the given\n",
      "        restraints on the number of antecedents and consequents.\n",
      "INFO: 23 rules found.\n"
     ]
    }
   ],
   "source": [
    "metric = \"confidence ratio\"\n",
    "confratio_grid = [0.5]*3 + [0.7]*3 + [0.8]*3 + [0.9]*3\n",
    "rulesupport_grid = [0.01]*12\n",
    "support_grid = [0]*12\n",
    "confidence_grid = [0.5, 0.4, 0.3]*4\n",
    "\n",
    "for i, (cr, rs, s, c) in enumerate(zip(confratio_grid, rulesupport_grid, support_grid, confidence_grid)):\n",
    "    tmp = rule_mining(df,\n",
    "                      use_colnames=True, min_rulesupport=rs,\n",
    "                      metric=metric, min_threshold=cr,\n",
    "                      max_antecedents=max_antecedents, max_consequents=max_consequents,\n",
    "                      min_confidence=c, min_support=s)\n",
    "    \n",
    "    tmp_path = path_dict[metric] + metric + \"_c\" + str(i+1) + \".csv\"\n",
    "    tmp.to_csv(tmp_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "creator": "admin",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
